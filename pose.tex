%-----------------------------------------------------------------------------------------------
\chapter{Pose estimation}\label{sect:pose}
%-----------------------------------------------------------------------------------------------

The goal of this project is to calculate camera pose using a fiducial marker.
To achieve this, a wide range of problems have to be solved including but not limited to marker design, detection, pose calculation.
This chapter will focus on the 3D pose estimation problem.
First, it's mathematical formulation will be presented.
Later some solutions to the problem will be summarised and compared.
Based on this comparison, an algorithm will be selected for use in this project.

Taking photos with a camera can be thought of as mapping 3D points from the scene being observed to the image plane of the camera.
This mapping depends on a number of parameters, some of which are tied to the camera, others depend on the viewpoint and angle.
Based on these parameters, a \textit{perspective projection model} can be constructed.
Mathematically, the model can described by a $P$ projection matrix.
It is a $3*4$ matrix, which gives the image point\footnote{Determined up to a scale factor} for any given world point in homogeneous coordinates.
\begin{equation}
	P = K [R|T]
	\label{eq:projMatrix}
\end{equation}
The $P$ matrix can be constructed as shown in \eqref{projMatrix}.

$K$ is a $3*3$ matrix describing the \textit{intrinsic camera parameters} or \textit{camera model}.
These parameters depend only the internals of the camera, thus are independent from the orientation or position.
Once these are measured for a camera, they can be reused later.
Some models use more variables to describe the internal workings of the camera, others are more simple.
The model used in this work is summarized by the camera matrix described in \eqref{intrinsicParams}.
\begin{equation}
	K =
	\begin{bmatrix}
		f_x & \gamma & u_0 \\
		0   & f_y    & v_0 \\
		0   & 0      & 1
	\end{bmatrix}
	\label{eq:intrinsicParams}
\end{equation}
The notation of the model is the following: $f_x$ and $f_y$ denote the focal lengths of the camera on the $x$ and $y$ axis, while $(u_0,v_0)$ is the principal point.
The principal point is the image point where the optical axis intersects with the image plane.
The parameter $\gamma$ is the skew of the camera, which will be neglected in this work, $\gamma = 0$ will be used.
The elements of matrix $K$, the intrinsic camera parameters are determined in the process of \textit{camera calibration}.

The other part of the projection matrix, $[R | T]$, describe the position and orientation of the camera in the world coordinate system.
These are called \textit{extrinsic parameters}, and depend on the current configuration (position) of the camera.
The matrix is constructed as shown in \eqref{extParams}.
\begin{equation}
	[R | T] =
	\begin{bmatrix}
		r_{11} & r_{12} & r_{13} & t_1\\
		r_{21} & r_{22} & r_{23} & t_2\\
		r_{31} & r_{32} & r_{33} & t_3\\
	\end{bmatrix}
	\label{eq:extParams}
\end{equation}
It is made up of two separate parts: $R$ describes the orientation, while $T$ the translation with respect to the origin of the world coordinate system.
These parameters are only valid while the camera remains stationary.
The calculation of matrix $[R|T]$ is the process of \textit{camera pose estimation}, which is also known as the \textit{perspective-n-point problem}.
The pose of a calibrated camera can be estimated using $n$ 3D points in the world frame and their corresponding $n$ points in the image plane.
The camera pose has 6 DOF: the rotation (roll, pitch, and yaw) and the translation.
In \eqref{extParams}, $R$ is the rotation matrix, and $T$ is the translation vector.

In the next section some solutions to the PnP problem will be presented.

%-----------------------------------------------------------------------------------------------
\section{Pose Estimation Algorithms}
%-----------------------------------------------------------------------------------------------

There have been many solutions to the perspective-n-point problem.
These varied in performance, accuracy and principle.
Some only gave solutions to the case of $n=3$, which is the simplest solution computationally, but is also very error prone. 
Others solved the problem for any number of point correspondences, but required that the points are not coplanar.
Still another class of algorithms worked on coplanar points.

As part of this project, multiple solutions were examined.
The goal is to select and algorithm that suits the needs of this project: the ideal algorithm has to be robust, reasonably accurate, and lightweight enough to be run on mobile devices.
As a general rule, iterative algorithms provide better accuracy, but they are also more computationally expensive.
On the other end of the spectrum the non-iterative solutions are usually less accurate, but are "cheaper" to use.

In the following sections will be short summaries of the algorithms considered for use in this project.
They will be compared and the most suitable will be selected.

%-----------------------------------------------------------------------------------------------
\subsection{EPnP}
%-----------------------------------------------------------------------------------------------

An efficient, non-iterative solution to the problem was described in \cite{Lepetit2008}.
The algorithm has a complexity of $O(n)$ for $n\geq4$, which is substantially more efficient than it's rivals.
The EP$n$P algorithm is more accurate than most non-iterative solutions, and it's also much faster than the iterative algorithms.

EP$n$P solves the pose estimation problem for $n\geq3$ point correspondences.
The core concept of the solution is that each $n$ \textit{reference point}\footnote{Points in the world frame} can be expressed as a weighted sum of 4 \textit{virtual control points}\cite{Lepetit2008} (actually, for the planar reference points, 3 virtual control point are enough).
This way the coordinates of these control points become the unknown variables of the problem.
The camera pose is later calculated from the control points.

A short summary of the process will be presented here, using the notation of the original paper\cite{Lepetit2008}.
First, the $n$ reference points in world coordinates ($p_i^w$) and their corresponding image reference points ($p_i^c$) are expressed as linear combinations of the virtual control points ($c_j^w, c_j^c$).
\begin{align}
	p_i^w = \sum_{j=1}^{4} \alpha_{ij} c_j^w \\ 
	p_i^c = \sum_{j=1}^{4} \alpha_{ij} c_j^c \\
	\sum_{j=1}^{4} \alpha_{ij} = 1
\end{align}
Note that the weights are normalised per reference point.
Also, all points are represented with homogeneous coordinates.

EP$n$P only calculates the extrinsic parameters of the projection.
It requires the intrinsic camera matrix to work, which will be noted with $K$.
With this in mind, the relationship between the image reference points and the world reference points can be written as
\begin{equation}
	s_i p_i^c = K \sum_{j=1}^{4} \alpha_{ij} c_j^c,
	\label{eq:controlPointProj}
\end{equation}
where $s_i$ is a scalar projective parameter.

The homogeneous coordinates of the image control points will be noted as follows.
\begin{equation}
	c_j^c = 
	\begin{bmatrix}
		x_j^c & y_j^c & z_j^c
	\end{bmatrix}^T
\end{equation}
With this notation, \eqref{controlPointProj} can be rearranged into the following form for each reference point.
\begin{align}
	\sum_{j=1}^{4} \alpha_{ij} f_x x_j^c + \alpha_{ij} (u_0 - u_i) \\
	\sum_{j=1}^{4} \alpha_{ij} f_y y_j^c + \alpha_{ij} (v_0 - v_i) 
\end{align}

Using the above two equations for each reference point, a homogeneous linear equation system $Mx = 0$ can be formed for the control points.
The $x$ vector is defined by
\begin{equation}
x = 
	\begin{bmatrix}
		{c_1^c}^T & {c_2^c}^T & {c_3^c}^T & {c_4^c}^T
	\end{bmatrix}^T,
\end{equation}
and the solution for control points will lie in the kernel of $M$.
Solving the system with the SVD method, the solution is expressed as:
\begin{equation}
	x = \sum_{i=1}^{N} \beta_i v_i
\end{equation}
Where $N$ is the number of the singular values of $M$ and $v_i$ is corresponding singular vector.

Using the solution obtained for the control points, the pose can be calculated, as for 3 or 4 points an exact solution exists to the problem.
It is shown in \cite{Lepetit2008} that the $R$ and $T$ matrices minimise the reprojection error between the world reference points and their corresponding image reference points.

%-----------------------------------------------------------------------------------------------
\subsection{An Iterative Solution}
%-----------------------------------------------------------------------------------------------

A commonly used iterative solution was given to the pose estimation problem in \cite{iterative}.
The proposed algorithm is globally convergent, has reasonable runtime (it usually converges in 5-10 iterations) and provides accurate results.
Opposed to the previous iterative algorithms, the method proposed in \cite{iterative} is based on minimizing an object-space collinearity error.
The algorithm successively improves an estimate of the rotation part of the pose, and then calculates an associated translation.

The iterative algorithm is based on solving the \textit{absolute orientation problem}.
Using the notation defined in \cite{iterative}, the problem can be formulated as follows.
Let $q_i$ be a reference point expressed in \textbf{camera-space coordinates}, while $p_i$ is the same point in \textbf{world coordinates}.
Then for each observed point \eqref{iterAbsOrient1} is true.
\begin{equation}
	q_i = R p_i + t
	\label{eq:iterAbsOrient1}
\end{equation}
Using 3 or more reference points, $R$ and $t$ (the rotation and translation between world- and camera coordinates) can be calculated by solving the following least-squares problem\cite{iterative}, where the solution is subject to the $R^TR = I$ constraint.
\begin{align}
	\min_{R,t} \sum_{i=1}^{n} ||R p_i + t - q_i ||^2
	\label{eq:iterLSProb}
\end{align}
The constrained LS problem can solved by singular value decomposition.
The process of the SVD solution is the following.

Let the centroids of the $p$ and $q$ points respectively be:
\begin{equation}
	\bar{p} = \frac{1}{n} \sum_{i=1}^{n} p_i,
	\bar{q} = \frac{1}{n} \sum_{i=1}^{n} q_i
\end{equation}
The sample cross-covariance matrix then can be expressed as:
\begin{equation}
	M = \sum_{i=1}^{n} (q_i - \bar{q})(p_i - \bar{p})^T
\end{equation}
It is shown in \cite{iterative} that if $R^*$ and $t^*$ minimize \eqref{iterLSProb}, they also satisfy the following equations.
\begin{align}
	R^* = \arg \max_R tr(R^TM)
	t^* = \bar{q} - R^*\bar{p}
\end{align}

Let $(U,\Sigma, V)$ be a SVD of $M$, that is $U^TMV=\Sigma$. Then the solution to \eqref{iterLSProb} is\cite{iterative}
\begin{equation}
	R^* = VU^T
	\label{eq:iterOrientSolution}
\end{equation}

The iterative part of the algorithm is heavily dependant on the above absolute orientation solution.
The algorithm is based on minimising the object-space collinearity error.
The error vector is defined in \eqref{iterErrFunc}, where $F_i$ is a projection operator defined as
\begin{equation}
	F_i = \frac{v_iv_i^T}{v_i^Tv_i}
\end{equation}
for each $v_i$ image point.
\begin{equation}
	e_i = (I - F_i)(Rp_i + t)
	\label{eq:iterErrFunc}
\end{equation}
Then, the magnitude of the error vector is minimized by the iterative algorithm.
\begin{equation}
	E(R,t) = \sum_{i=1}^{n} ||e_i||^2 = \sum_{i=1}^{n} ||(I - F_i)(Rp_i + t)||^2 
	\label{eq:iterErrFuncMagn}
\end{equation}
The above formula is quadratic in $t$, so the translation can be calculated in closed form given any fixed $R$ rotation.
Since the optimal translation can be expressed as a function of $R$, \eqref{iterErrFuncMagn} can be rewritten in a form closely resembling the absolute orientation problem.
The camera space reference points also can be expressed as a function of $R$.
The reformulated error function is the following.
\begin{equation}
	E(R) = \sum_{i=1}^{n} || Rp_i + t(R) - q_i(R) ||^2
\end{equation}
The above formula, unlike \eqref{iterLSProb}, can not be solved for $R$ in closed form, as the sample cross-covariance matrix also depends on $R$ itself.
However, $R$ can be calculated iteratively.
If the $k$th estimate of the orientation is known (noted with $R^{(k)}$), the next estimate, $R^{(k+1)}$ can be calculated by solving the following absolute orientation problem.
\begin{align}
	R^{(k+1)} &= \arg \min_R \sum_{i=1}^{n} || Rp_i + t^{(k)} - F_iq_i^{(k)} ||^2 \\
			  &= \arg \max_R tr(R^TM(R^{(k)})) 
\end{align}
In each iteration, the translation is also calculated, as it only depends on the orientation estimate.

Solving the above formula iteratively will converge on the correct camera pose\cite{iterative}.
The algorithm is globally convergent, so any rotation can be chosen to initialise the process.

%-----------------------------------------------------------------------------------------------
\subsection{Robust Pose Estimation from a Planar Target}
%-----------------------------------------------------------------------------------------------

The \textit{Robust Pose Estimation for Planar Targets} algorithm was published in \cite{robust}.
It is an iterative method for calculating the camera pose that addresses some problems of the already existing solutions.
Namely, all previous iterative algorithm suffer from \textit{pose ambiguities}\cite{robust}.
These can cause jumps in the measured camera pose in consecutive images, which are not tolerable in some applications.
Also, this solution is optimised for planar targets.
This property makes it a good candidate for this project.
This method is based on the iterative approach described above, the same notations will be used here also.

It is shown in \cite{robust} that multiple local minima of the error function\footnote{The same, object space error function is used as in the above described iterative method} can exist even in the noise-free scenario.
The existence of such local minima depends on all parameters of the error function.
In the noise-free case, the global minimum of the error is 0 for the real camera pose.
If the measurements are burdened with noise, the error function is always greater than zero, but the real solution should have the lowest level of error out the multiple possible minima.
This algorithm provides a way to select the correct solution from the possibilities, eliminating pose jumps.

The algorithm is based on two major observations\cite{robust}:
\begin{itemize}
	\item Sometimes there are two distinct local minima, depending on the actual configuration ($R, t, p_i, v_i$)
	\item The correct solution should have the lower error magnitude.
\end{itemize}

The algorithm proposed by \cite{robust} will be shown below.
\begin{enumerate}
	\item Estimate a first pose $\hat{P}_1 = (\hat{R}_1, \hat{t}_1)$ by applying any existing iterative pose estimation algorithm. $\hat{P}_1$ is one local minimum of the error function. The goal of this algorithm is to analytically derive an estimate of the second local minimum, if such a minimum exists.
	\item Transform the coordinate system to get $\tilde{P}_1 = (\tilde{R}_1, \tilde{t}_1)$.
	\item Estimate $\tilde{R}_z$ to obtain the transformed system and the parameters of the first pose ($\tilde{\gamma}_1$ and $\tilde{\beta}_1$).
	\item Fix $\gamma = \tilde{\gamma}_1$, and estimate all local minima of the error function for the parameters $\beta$ and $\tilde{t}$.
	\item Undo the transformations of step 1 and step 2 for all local minima to obtain poses $\hat{P}_i$.
	\item Use all poses $\hat{P}_i$ as a start value for the iterative pose estimation algorithm to get final poses $P_i^*$
	\item Decide the final and correct pose, which has the lowest error magnitude.
\end{enumerate}

For detailed description of all transformations and processing steps, see \cite{robust}.

%-----------------------------------------------------------------------------------------------
\section{Comparison}
%-----------------------------------------------------------------------------------------------

The algorithms considered for use (\textit{EPnP}, \textit{the iterative approach} and \textit{the robust pose estimation}) each have desirable properties.
Each excel in different aspects, but their comparative advantages also have their price.
Considering the probable use-cases of this project, the following properties of an algorithm are important.
First of all, the pose estimation has to be \textit{robust}.
Jumps in the detected pose are undesirable.
The computational efficiency of the solution is also important, as in the later stages of this project use in embedded systems or mobile devices is planned.
Naturally, the accuracy of the algorithm is also considered.
The more accurate the measured pose, the better, however robustness and performance can not be sacrificed for it.

When computational efficiency is concerned, EP$n$P is the clear winner.
It's complexity is linear in the number of corresponding point pairs: it is $O(n)$ for $n\geq4$.
Usually, iterative methods are quite costly: they range from $O(n^2)$ to $O(n^8)$.
The iterative algorithm described in \cite{iterative} is actually quite efficient, usually converging quickly.
In optimal cases, it is comparable to the performance of EP$n$P, but if it is not initialised correctly it can get stuck in an incorrect local minimum.
Performance-wise the robust pose estimation algorithm is the most expensive.
It is based on an iterative approach, and further operations are done to provide robustness.
However, it also is quite usable, as the implementation tried and used in this project is based on \cite{iterative}.

Finding a robust enough algorithm for this project is challenging.
In this paper the use of planar markers is discussed.
Planar reference points are a critical configuration for the perspective-n-point problem.
It is shown in \cite{robust} that the probability choosing the correct pose using iterative algorithms can be as low as $60\%$.
The iterative approach described here may result in pose jumps in $40\%$ of the cases using a planar target.
This statistic is significantly improved by the robust pose estimation algorithm.

Finally, the accuracy of the solutions have to be assessed.
The robust and the iterative methods provide the same level of precision, as they are basically the same.
Their error remains below $5\%$ even for noisy images.
Compared to that, the EP$n$P method performs worse, reaching up to $10\%$ rotation error.
However, it can be improved if necessary by applying Gauss-Newton optimisation for the detected pose, or using the detected pose as initialisation for the iterative algorithm.

Based on the above observations, both EP$n$P and the robust pose estimation algorithm can be recommended for use.
For the development phase, the robust solution will be used.
However, depending on the available resources in mobile platforms and embedded systems, EP$n$P can provide a good alternative.
